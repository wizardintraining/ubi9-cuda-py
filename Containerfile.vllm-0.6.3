# explicitly set the list to avoid issues with torch 2.2
# see https://github.com/pytorch/pytorch/pull/123243
ARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'
# Override the arch list for flash-attn to reduce the binary size
ARG vllm_fa_cmake_gpu_arches='80-real;90-real'

FROM quay.io/stewhite/ubi9-cuda-py:0.1.0-devel as vllm-install

ENV CUDA_VERSION 12.4.1
ENV PYTHON_VERSION 3.11

ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}
ENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}

WORKDIR /opt/app-root/src

USER 1001

COPY --chown=1001:0 requirements-vllm-0.6.3.txt ./requirements.txt

#Build env setup
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads

RUN pip install --no-cache-dir -r requirements.txt wheel && \
    rm -f requirements.txt && \
    # Install flash-attn from PyPI \
    pip install --no-cache-dir flash-attn~=2.5.9 --no-build-isolation && \
    pip install --no-cache-dir vllm==0.6.3 vllm-flash-attn vllm-nccl-cu12 && \
    # Correction for FIPS mode \
    sed -i s/md5/sha1/g /opt/app-root/lib64/python3.11/site-packages/triton/runtime/jit.py && \
    # Fix permissions to support pip in Openshift environments \
    chmod -R g+w /opt/app-root/lib/python3.11/site-packages && \
    fix-permissions /opt/app-root -P

##################
# vLLM container #
##################

FROM quay.io/stewhite/ubi9-cuda-py:0.1.0 as runtime-container

ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}
ENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}

WORKDIR /opt/app-root/src

COPY --from=vllm-install --chown=1001:0 /opt/app-root/lib64/python3.11/site-packages /opt/app-root/lib64/python3.11/site-packages
COPY --from=vllm-install --chown=1001:0 /opt/app-root/src/.config/vllm/nccl/cu12/libnccl.so.2* /usr/local/lib/libnccl.so.2

# Fix VLLM_NCCL_SO_PATH
ENV VLLM_NCCL_SO_PATH=/usr/local/lib/libnccl.so.2

USER 1001

EXPOSE 8000

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]

