ARG LLAMA_CPP_PYTHON_VERSION=0.3.1
# ARG LLAMA_CPP_VERSION=c919d5d


###################################
# Build llamma-cpp-py for image  #
###################################
FROM quay.io/stewhite/ubi9-cuda-py:0.1.0-devel as llama-cpp-build

ENV LLAMA_CPP_PYTHON_VERSION=${LLAMA_CPP_PYTHON_VERSION}

USER 0
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1

USER 1001

################################
# # Manual Install code pull   #
# RUN git clone --depth 1 --branch v${LLAMA_CPP_PYTHON_VERSION} --recurse-submodules \
#    https://github.com/abetlen/llama-cpp-python.git &&\
#    cd llama-cpp-python/vendor/llama.cpp && \
#    git checkout ${LLAMA_CPP_VERSION} 
#  # required for direct updates to LLAMA_CPP_VERSION
################################


RUN python3 -m venv --upgrade-deps ${APP_ROOT}  && \
    # Turn off manual install \
    # CMAKE_ARGS="-DGGML_CUDA=on -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" \
    CMAKE_ARGS="-DGGML_CUDA=ON -DLLAMA_NATIVE=OFF -DGGML_NO_CCACHE=1" \
    pip install --upgrade --verbose 'llama-cpp-python[server]'"${LLAMA_CPP_PYTHON_VERSION:+~=LLAMA_CPP_PYTHON_VERSION}"

USER 0
    # Fix permissions to support pip in Openshift environments
RUN chmod -R g+w ${APP_ROOT}/lib/python3.11/site-packages && \
    fix-permissions ${APP_ROOT} -P
USER 1001

##################
# run container #
##################
FROM quay.io/stewhite/ubi9-cuda-py:0.1.0 as runtime-container

ENV LLAMA_CPP_PYTHON_VERSION=${LLAMA_CPP_PYTHON_VERSION}
ENV GGML_CUDA_ENABLE_UNIFIED_MEMORY=1

ARG PORT=8080
ENV PORT=${PORT} \
    GPU_LAYERS=-1 \
    CONTEXT_SIZE=16192 \
    MODEL_FOLDER=${APP_ROOT}/data \
    MODEL=granite-20b-code-instruct.Q4_K_M.gguf \
    MODEL_NAME=ibm/granite-20b-code-instruct

EXPOSE ${PORT}
VOLUME ["/opt/app-root/data"]

COPY --from=llama-cpp-build --chown=1001:0 ${APP_ROOT}/lib64 ${APP_ROOT}/lib64
COPY --from=llama-cpp-build --chown=1001:0 ${APP_ROOT}/bin ${APP_ROOT}/bin

USER 0

RUN mkdir -p ${MODEL_FOLDER} \
    && chmod -R g+w ${MODEL_FOLDER} \
    && fix-permissions ${APP_ROOT} -P

USER 1001
# ENTRYPOINT ["python3", "-m", "llama_cpp.server"]

CMD python3 -m llama_cpp.server --host 0.0.0.0 --port ${PORT} \
    --n_gpu_layers ${GPU_LAYERS} --n_ctx ${CONTEXT_SIZE} \
    --model ${MODEL_FOLDER}/${MODEL} --model_alias ${MODEL_NAME}
